{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc73f20a-dc69-4872-a1e5-05c9068ceb9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task 7: Label Propagation for Node Classification\n",
    "Given a network with labels on some nodes, how do we assign labels to all\n",
    "other nodes in the network?\n",
    "\n",
    "We can potentailly leverage that correlations exist in networks - nearby nodes are similar. Correlations exist becasue : \n",
    "\n",
    "- Homophily: The tendency of individuals to associate and bond with similar others\n",
    "\n",
    "- Influence: Social connections can influence the individual characteristics of a person. \n",
    "\n",
    "To leverage this correlation observed in networks to predict node labels,we can classify the label of a node $v$ in network using\n",
    "\n",
    "- Features of $v$\n",
    "- Labels of the nodes in $v$’s neighborhood\n",
    "- Features of the nodes in $v$’s neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416dbeed-c853-4c37-a023-e1357af71eee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Relational Clasification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36b04d-6648-44e6-afc7-77d0eea368b0",
   "metadata": {},
   "source": [
    "### Probabilistic Relational Classifier\n",
    "\n",
    "Class probability $Y_v$ of node $v$ is a weighted average of class probabilities of its neighbors.\n",
    "\n",
    "1. For labeled nodes $v$, initialize label $Y_v$ with ground-truth label $Y_v^*$. For unlabeled nodes, initialize $Y_v = 0.5$.\n",
    "2. For each node $v$ and label $c$\n",
    "\n",
    "    $$P(Y_v=c)= \\frac{1} {\\sum_{(v,u) \\in E} A_{v,u}}\\sum_{(v,u) \\in E} A_{v,u}P(Y_u=c)$$\n",
    "\n",
    "    If edges have strength/weight information, $A_{v,u}$ can be the edge weight between $v$ and $u$. $P(Y_u=c)$ is the probability of node $u$ having label $c$\n",
    "\n",
    "3. Update all nodes in a random order until convergence or until maximum number of iterations is reached. \n",
    "\n",
    "There are two issues with the method:\n",
    "- Convergence is not guaranteed\n",
    "- Node feature information is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a9ea3-5f34-414e-84d6-075a67e44a50",
   "metadata": {},
   "source": [
    "## Iterative Clasification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c450dd1-7507-47c0-bcc9-a1585768289b",
   "metadata": {},
   "source": [
    "### Interative Classifier\n",
    "To leverage node level features, iterative classification classify\n",
    "node $v$ based on its attributes $f_v$ as well as\n",
    "labels $z_v$ of neighbor set $\\mathbf{N}_v$.\n",
    "\n",
    "This method involves training two classifiers on **labelled training data**:\n",
    "- **Base classifier** $\\phi_1 (f_v)$ predicts node label $Y_v$ based on node feature vector $f_v$. \n",
    "- **Relational classifider** $\\phi_2 (f_v,z_v)$ predicts label $Y_v$ based on node feature vector $f_v$ and summary $z_v$ of labels of $v$’s neighbors.\n",
    "\n",
    "When doing the inferences:\n",
    "\n",
    "1. For each node in the test data\n",
    "    - set labels $Y_v$ based on the base classifier $\\phi_1 (f_v)$\n",
    "    - compute $z_v$ \n",
    "    - predict the labels with $\\phi_1(f_v,z_v)$\n",
    "2. Repeat for each node $w$:\n",
    "    - update $z_v$ based on $Y_u$ for all $u \\in \\mathbf{N}_v$\n",
    "    - update $Y_v$ based on the new $z_v(\\phi_2)$\n",
    "    \n",
    "3. Update all nodes in a random order until convergence or until maximum number of iterations is reached. Again, convergence is not guaranteed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39faeeb3-120e-4580-b8e2-386ff8ed2058",
   "metadata": {},
   "source": [
    "## Collective Clasification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce47a7-da17-4fe7-b132-aa7bb3f6d9cc",
   "metadata": {},
   "source": [
    "### Correct & Smooth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ccb43d-8bc9-4200-ab6c-2b865b478676",
   "metadata": {},
   "source": [
    "Correct & Smooth takes the following steps\n",
    "1. Train a base predictor that predict soft labels (class probabilities) over all nodes.\n",
    "    - Labeled nodes are used for train/validation data.\n",
    "    - Base predictor can be simple. For example, Linear model/Multi-Layer-Perceptron(MLP) over node features\n",
    "\n",
    "2. Given a trained base predictor, we apply it to obtain soft labels for all the nodes. We expect these soft labels to be decently accurate.\n",
    "\n",
    "3. 2-step procedure to postprocess the soft predictions.\n",
    "    - **Correct step**: The degree of the errors of the soft labels are biased. We need to correct for the error bias.\n",
    "        - ***Compute training errors of nodes***. The training error is caculated as ground-truth label minus soft label.Defined as 0 for unlabeled nodes.\n",
    "        - ***Diffuse training errors $E^{(0)}$ along the edges***. The assumption here is that errors are simiar for nearby nodes.\n",
    "        \n",
    "        \n",
    "            $$E^{(t+1)}\\leftarrow (1-\\alpha) \\cdot E^{(t)} +\\alpha \\cdot \\tilde A E^{(t)}$$\n",
    "            \n",
    "          Where $\\alpha$ is a hypterparamter. $\\tilde A$ is the normalized diffusion matrix. It is defined as follows:\n",
    "          \n",
    "            $$ \\tilde  A \\equiv D^{-1/2} A D^{-1/2} $$\n",
    "          \n",
    "          Where $A$ be the adjacency matrix and Let $D \\equiv \\mathrm{Diag}(d_1,..,d_N)$ be the degree matrix.\n",
    "          \n",
    "          For more details on this steps, Please refer to [Zhu et al. ICML 2013](https://mlg.eng.cam.ac.uk/zoubin/papers/zgl.pdf).\n",
    "        - ***Add the scaled diffused training errors into the predicted soft labels***\n",
    "          \n",
    "    - **Smooth step**: The predicted soft labels may not be smooth over the graph. We need to smoothen the corrected soft labels along the edges. The assumption here is that neighboring nodes tend to share the same labels. \n",
    "        - ***Diffuse label $Z^{(0)}$ along the graph structure*** \n",
    "           \n",
    "           $$Z^{(t+1)}\\leftarrow (1-\\alpha) \\cdot Z^{(t)} +\\alpha \\cdot \\tilde A Z^{(t)}$$\n",
    "         \n",
    "           \n",
    "C&S achieves strong performance on semisupervised node classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30924cc9-9127-400b-81cc-ac38e1a0627d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1\\][Label Propagation for Node Classification Youtube. Stanford CS224W: Machine Learning with Graphs | 2021](https://www.youtube.com/watch?v=6g9vtxUmfwM&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=14)\n",
    "\n",
    "\\[2\\][Label Propagation for Node Classification Slides. Stanford CS224W: Machine Learning with Graphs | 2021](http://snap.stanford.edu/class/cs224w-2021/slides/05-message.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
