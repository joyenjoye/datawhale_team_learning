{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a959fe60-3179-4635-b051-bb9b9fcbae3c",
   "metadata": {},
   "source": [
    "# Task 8: Graph Neural Networks - GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3a983-37ed-4ac7-8d34-8d9852c3a790",
   "metadata": {},
   "source": [
    "In Task 2, we learnt traditional feature based methods - for a given input graph, node, link and graph-level features are extracted so that they can be feed into a model (SVM, neural network) that maps features to target labels.\n",
    "\n",
    "In Task 4, we learnt graph representation learning which learns task-idependent features for downstream models efficiently. It uses a ***shallow*** **Encoder** to map nodes to emebdings and **Decoder** to map embeddings to similarity Score. \n",
    "\n",
    "The limitation of shallow emebedding methods are as follows:\n",
    "- The complexity of $O(|V|)$ as there is no sharing of paramters between nodes, and every nodes has its own unique embedding\n",
    "- Inherently transductive and cannot generte emebeddings that not seen during training.\n",
    "- Node features are not incorporated.\n",
    "\n",
    "\n",
    "In this task, we learn how to use deep learning methods (graph neural networks, GNNs) to get a deep encoder to map nodes to embeddings. \n",
    "\n",
    "Essentially, the deep encoder is a multiple layers of non-linear transformations based on graph structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2eb07-2b11-4b83-bbb9-7552faafdb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
