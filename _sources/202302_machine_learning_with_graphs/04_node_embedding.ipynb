{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64651a09-8f24-40b5-af65-6d8447df271a",
   "metadata": {},
   "source": [
    "# Task 4: Node Embeddings\n",
    "\n",
    "In Task 2, we learnt traditional feature based methods - for a given input graph, node, link and graph-level features are extracted so that they can be feed into a model (SVM, neural network) that maps features to target labels.\n",
    "\n",
    "Graph representation learning alleviates the need to do feature engineering manully, but instead automatically learn the features.\n",
    "\n",
    "The goal for graph representation learning here is to learn  task-idependent features for downstream models efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24a9ab-b60a-4a6a-a121-89e2d71a2a99",
   "metadata": {},
   "source": [
    "The task of learning node embeddings is to map nodes to an embedding space so that similarity of embeddings between nodes indicates their similarity in the network. In other words, the embedding should be able to capture the nework information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fce6f-f92f-4a6f-b9b5-41709e1a2a70",
   "metadata": {},
   "source": [
    "Assume we have a graph $G$:\n",
    "- $V$ is the vertex set. \n",
    "- $A$ is the adjacency maxtrix(assume binary). \n",
    "\n",
    "The goal is to encode nodes so that similarity in the embedding space approximates similarity in the graph.\n",
    "\n",
    "There are two key components in the above process:\n",
    "\n",
    "- **Encoder** maps from nodes to embeddings. \n",
    "\n",
    "    \n",
    "    $$\\mathrm{ENC}(v) = z_v$$\n",
    "\n",
    "- **Decoder** maps from embedding to simiarliy score for node $u$ and $v$.\n",
    "\n",
    "    \n",
    "    $$\\mathrm{DEC} = z_v^T z_u$$\n",
    "    \n",
    "    here is the dot product between embeddings for node $v$ and node $u$.\n",
    "  \n",
    "So the problem here is to optimize the parameters of the encoder so that\n",
    "\n",
    "$$\\mathrm{similarity}(u,v) \\approx z_v^T z_u$$\n",
    "\n",
    "Here $\\mathrm{similarity}(u,v)$ refers to the similarity of the node is the original netowrk.\n",
    "\n",
    "To do the above optimation, we need to define both encoder and node similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24533ff3-312e-48a3-a5e1-aa427be0c397",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shallow Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872e8ca-dab9-4c79-91f0-690ad3425dcd",
   "metadata": {},
   "source": [
    "Simplests encoding approach is to treat encoder as just an embedding look up table.\n",
    "\n",
    "$$\\mathrm{ENC}(v) = z_v = Z*v$$\n",
    "\n",
    "Where $Z \\in \\mathbb{R}^{d*|v|}$ is a matrix where each column is node embeding, and $v\\in\\mathbb{I}^{|v|}$ is indicator vector, all zeros except a one in column indicating node $v$.\n",
    "\n",
    "In such cases, each node is assigned a unique emebedding vector, and we can directly optimize the embedding of each node $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ca890-34f0-42dc-9f47-11d0d1a3d99a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Node Similarity\n",
    "\n",
    "Intuitively, if the two nodes are linked, or share neighbors, or have similar structural roles, they are more likely to have similar emebeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7394b-900a-40ca-9d3f-d5d2fc0660c5",
   "metadata": {},
   "source": [
    "### Node Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49189fb0-0f0f-4356-8275-8aa31fccc28e",
   "metadata": {},
   "source": [
    "Simplest node similarity: node $v$ and node $u$ are similar if they are connected by an edge. This means: \n",
    "\n",
    "$$z_v^T z_u = A_{u,v}$$\n",
    "\n",
    "which is the $(u,v)$ entry of the graph adjacency matrix $A$. Therefore,\n",
    "\n",
    "$$Z^TZ = A$$\n",
    "\n",
    "Exact factorization $A=Z^TZ$ is generally not possible. However, we can learn $Z$ approximately. \n",
    "\n",
    "Specifically, we optimize $Z$ such that it minimizes the L2 norm (Frobenius norm) of $A-Z^TZ$. The objective function is thus as follows:\n",
    "\n",
    "$$\\min_Z\\lVert A-Z^TZ\\rVert_2$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af4d21-6e82-4033-9f2f-c628a9cd6c6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6ef80-af5b-42c1-b12f-2abde6edf30b",
   "metadata": {},
   "source": [
    "One way is to define node similarity using random walks.\n",
    "\n",
    "Given a graph and a starting point, we select a neighbor of it at random, and move to this neighbor, then we select a neighbor of this point at radom and move to it and repeat... The random sequence of points visited this way is a **random walk** on the graph.\n",
    "\n",
    "The probability that node $v$ and node $u$ co-occur on the random walk over graph can be used to measure the node simiarlity between node $v$ and node $u$. The intuition is that If random walk starting from node $u$ visits $v$ with high probability, $u$ and $v$ are similar (high-order multi-hop information).As such, we can write the follows:\n",
    "\n",
    "$$P_R(v|u) \\approx z_v^T z_u$$\n",
    "\n",
    "The reasons that why should we use random walks for node similarity:\n",
    "- **Expressivity**:  Flexible stochastic definition of node similarity that incorporates both local and higher-order neighborhood information.\n",
    "- **Efficiency**: Do not need to consider all node pairs when training; only need to consider pairs that co-occur on random walks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f80e8f-9f15-4eb0-85cd-f8e509a94e31",
   "metadata": {},
   "source": [
    "#### Random walk Strategies\n",
    "There are different strategies that we can run random walk:\n",
    "\n",
    "- Fixed-length, unbiased random walk\n",
    "    - DeepWalk: [(Perozzi et al., 2013)](https://arxiv.org/abs/1403.6652)\n",
    "    - The issue is that such notion of similarity is too constrained.\n",
    "- Biased random walks:\n",
    "    - Node2Vec: [(Grover and Leskovec, 2016)](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf). \n",
    "    - Based on node attributes (Dong et al., 2017).\n",
    "    - Based on learned weights (Abu-El-Haija et al., 2017)\n",
    "    \n",
    "- Alternative optimization schemes:\n",
    "    - Directly optimize based on 1-hop and 2-hop random walk probabilities (as in LINE from Tang et al. 2015).\n",
    "\n",
    "- Network preprocessing techniques:\n",
    "    - Run random walks on modified versions of the original network (e.g., Ribeiro et al. 2017’s struct2vec, Chen et al.2016’s HARP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d128eca6-0358-44dd-8886-4e09ff760a9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9409789-bb67-4977-8f38-b0d76fc1e0a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Now, we can formally definie the optimization. For given $G = (V,E)$, the goal is to learn a mapping $f:u \\rightarrow \\mathbb{R}^d: f(u) = \\mathbf{z}_u$ to maximize the Log-likelihood objective:\n",
    "\n",
    "$$\\max_f\\sum_{u \\in V}\\log P(N_R(u)|\\mathbf{z}_u)\\tag{1}$$\n",
    "\n",
    "Where $N_R(u)$ is the neighborhood of node $u$ by strategy $R$.\n",
    "\n",
    "The optimization takes the following steps:\n",
    "\n",
    "- Run short fixed-length random walks starting from each node $u$ in the graph using some random walk strategy $R$.\n",
    "- For each node $R$ collect $N_R(u)$, the multiset of nodes visited on random walks starting from $u$.\n",
    "- Optimize embeddings $Z_u$ so that for a given node $u$, predict its neighbors $N_R(u)$.\n",
    "\n",
    "\n",
    "The maximization of the above is equivalent to minimize the following:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{u \\in V}\\sum_{v \\in N_R(u)}-\\log P(v|\\mathbf{z}_u)\\tag{2}\\label{eq:loss_function}$$\n",
    "\n",
    "Here, we are optimizing emebding $Z_u$ to maximize the likelihood of random walk co-occurrences. \n",
    "\n",
    "Where $P(v|\\mathbf{z}_u)$ can be parmeterize using softmax:\n",
    "\n",
    "$$\n",
    "P(v|\\mathbf{z}_u) = \\frac{\\exp(z_u^T z_v)}{\\sum_{n \\in V}\\exp(z_v^T z_n)} \\tag{3}\\label{eq:likelihood}\n",
    "$$\n",
    "\n",
    "Plug $\\eqref{eq:likelihood}$ into $\\eqref{eq:loss_function}$, we get the following loss function:\n",
    "\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{u \\in V}\\sum_{v \\in N_R(u)}-\\log \\frac{\\exp(z_u^T z_v)}{\\sum_{n \\in V}\\exp(z_v^T z_n)}\\tag{4}\\label{eq:loss_function_final}$$\n",
    "\n",
    "But doing this naively is too expensive becaue nested sum over nodes gives complexity of $\\mathrm{O}(|V|^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1e93a-9276-4782-a836-f54e059eeae0",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfa861-c58b-4cfa-a5a6-7eeea212024d",
   "metadata": {},
   "source": [
    "\n",
    "To address this, we can consider to approximate the the normalization term as follows:\n",
    "\n",
    "$$\\log \\frac{\\exp(z_u^T z_v)}{\\sum_{n \\in V}\\exp(z_v^T z_n)} \\approx log\\big(\\sigma(z_u^T z_v)\\big) - \\sum_{i=1}^{k}log\\big(\\sigma(z_u^T z_{n_i})\\big), n_i \\sim P_V$$\n",
    "\n",
    "\n",
    "Instead of normalizing w.r.t. all nodes, just normalize against $k$ random \"negative samples\" $n_i$\n",
    "\n",
    "The above approximation is called negative sampling it is a form of noise contrastive estimation(NCE) which approximate the maximimation of the log probability of softmax. \n",
    "\n",
    "The new formulation correspons to using the logistic regression to distinguish the target node $v$ from nodes $n_i$ sampled from the background distribution $P_v$. For more details on this, refer to [oldberg, Y. and Levy, O., 2014](https://arxiv.org/pdf/1402.3722.pdf).\n",
    "\n",
    "Considerations for negative sampling:\n",
    "- Higher $k$ gives more robust estimates\n",
    "- Higher $k$ corresponds to higher bias on negative events. In practice $k =5-20$ .\n",
    "- Can negative sample be any node or only the nodes not on the\n",
    "walk? People often use any nodes (for efficiency). However, the\n",
    "most “correct” way is to use nodes not on the walk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8908034-9ee0-41de-8e2c-ed673a80132f",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed19ab8-9645-4c78-beeb-b204f4915e8e",
   "metadata": {},
   "source": [
    "Now that we have the loss function, and we need to optimize (minimize) it.\n",
    "\n",
    "**Gradient Descent** is a simple way to minimize $\\mathcal{L}$ :\n",
    "- Initialize $z_u$ at some randomized value for all nodes $u$.\n",
    "- Iterate until convergence:\n",
    "    - For all $u$, compute the derivative \n",
    "    \n",
    "        $$ \\frac{\\partial \\mathcal{L}}{\\partial z_u}$$\n",
    "    \n",
    "    - For all $u$, make a step in reverse direction of derivative: \n",
    "    \n",
    "        $$ z_u \\leftarrow z_u - \\eta \\frac{\\partial \\mathcal{L}}{\\partial z_u}$$\n",
    "    \n",
    "        where $\\eta$ is lthe earning rate.\n",
    "    \n",
    "**Stochastic Gradient Descent** evaluates it for each individual training example instead of evaluating gradients over all examples.\n",
    "- Initialize $z_u$ at some randomized value for all nodes $u$.\n",
    "- Iterate until convergence: \n",
    "\n",
    "    $$\\mathcal{L}^{(u)} = \\sum_{v \\in N_R(u)}-\\log P(v|\\mathbf{z}_u)$$\n",
    "    \n",
    "    - Sample a node $u$, for all $v$, compute the derivative \n",
    "    \n",
    "        $$ \\frac{\\partial \\mathcal{L}^{(u)}}{\\partial z_v}$$\n",
    "    \n",
    "    - For all $v$, update\n",
    "    \n",
    "    $$ z_v \\leftarrow z_v - \\eta \\frac{\\partial \\mathcal{L}^{(u)}}{\\partial z_v}$$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4cebfc-33b1-40cd-8f15-08bb2ccbaeaa",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc80cc-5c6f-4abc-b57a-054881ef01dd",
   "metadata": {},
   "source": [
    "Limitations of node embeddings via matrix factorization and random walks\n",
    "- Cannot obtain embeddings for nodes not in the training set\n",
    "- Cannot capture structural similarity\n",
    "- Cannot utilize node, edge and graph features\n",
    "\n",
    "To address these limitations, deep representation learning and graph neural networks can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8cef1-cac5-40d8-90d4-302ca4929ad8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How to Use Embedding\n",
    "\n",
    "For **node-level tasks** such as clustering/community dectection, node classification, we can just directly use the node embedding $z_i$ for a given node $i$.\n",
    "\n",
    "For **link-level tasks** that predict edge$(i,j)$ base on  $(z_i,z_j)$, we can concatenate, avg, product, or take a difference\n",
    "between the embeddings to get the link embedding.\n",
    "\n",
    "For **graph-level tasks** that classsify graphs into different classes, we can get graph embedding through the following 3 approaches. Examples of graph level tasks are classifying toxic vs. non-toxic molecules, and identifying anomalous graphs. \n",
    "- **Approach 1**: Embed notdes and aggregate node embeddings.  \n",
    "    - Run a standard node embedding technique on the (sub)graph $G$.\n",
    "    - Then just sum (or average) the node embeddings in the (sub)graph $G$.\n",
    "    \n",
    "    It is simple but efficient and was used by [Duvenaud et al., 2016](https://arxiv.org/abs/1509.09292) to classify molecules based on their graph structure.\n",
    "\n",
    "- **Approach 2**: Introduce and embed virtual node.\n",
    "    - Create super-node that spans the (sub) graph and then embed that node.\n",
    "    - use the virtual node embedding as graph embedding.\n",
    "\n",
    "    It was proposed by [Li et al., 2016](https://arxiv.org/abs/1511.05493) as a general\n",
    "    technique for subgraph embedding.\n",
    "\n",
    "- **Approach 3**: hierarchically embeddings\n",
    "    - Hierarchically cluster nodes in graphs,\n",
    "    - Then sum(or average) the node embeddings according to these clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92819d5-feeb-40be-83d3-451002092991",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1\\][Node Embeddings Youtube. Stanford CS224W: Machine Learning with Graphs | 2021](https://www.youtube.com/watch?v=rMq21iY61SE&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=7)\n",
    "\n",
    "\\[2\\][Node Embeddings Slides. Stanford CS224W: Machine Learning with Graphs | 2021](http://web.stanford.edu/class/cs224w/slides/03-nodeemb.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
