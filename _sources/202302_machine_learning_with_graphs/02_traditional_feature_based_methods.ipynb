{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44791e67-9645-4e2f-b071-1a57ff1a4cec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task 2: Traditional Feature-based Methods\n",
    "\n",
    "Using effective features over graphs is the key to achieving good test performance. Traditional Machine Learning Pipeline manually designs features for nodes/links/graphs. \n",
    "\n",
    "This posts will cover traditional features for node/link/graph level prediction for undirected graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d10fd3-64f4-4cd8-9522-b1c2656afa38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Node Level Tasks and Features\n",
    "\n",
    "[Node classification](https://neo4j.com/developer/graph-data-science/node-classification/) is a supervised machine learning (ML) approach whereby existing nodes with known classes can be used to train a model that will learn the classes for nodes where they are unknown.\n",
    "\n",
    "There are different ways to characterize the structure and position of a node in the network.\n",
    "\n",
    "**Importance based features** capture the importance of a node in graph. This type of features is useful for predicting influential nodes in a graph.e.g.predicting celebrity users in a social network.\n",
    "- Node degree\n",
    "- Different node centrality measures\n",
    "\n",
    "**Structure based features** : Capture topological properties of local neighborhood around a node.Useful for predicting a particular role a node plays in a graph. e.g. Predicting protein functionality in a\n",
    "protein protein interaction network.\n",
    "- Node degree\n",
    "- Clustering coefficient\n",
    "- Graphlet count vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11474b4-c716-4ba9-8e68-84b56a16e25f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Node Degree\n",
    "\n",
    "The degree $k_v$ of node $v$ is the number of edges (neighboring nodes) the node has. Treats all neighboring nodes equally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58774fa5-ae2a-4f6d-8ef6-2bd6a93e47ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Node centrality\n",
    "\n",
    "Node centrality $C_v$ takes the node importance in a graph into account. There are different ways to model importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7836a7a-bd94-44ab-a290-c80fce2c2c72",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Engienvector centrality\n",
    "\n",
    "Engienvector centrality models the centrality of node $v$ as the sum of the centrality of neighboring nodes:\n",
    "\n",
    "$$c_v = \\frac{1}{\\lambda}\\sum_{u \\in N(v)}{c_u}$$\n",
    "\n",
    "Where $\\lambda$ is normalization constant. It will turn out to be the largest eigenvalue of A.\n",
    "\n",
    "Rewrite the recursive equation in the matrix form.\n",
    "\n",
    "$$\\lambda c = A\\mathbb{c}$$\n",
    "\n",
    "Where A is adjacent matrix, $A_uv=1$ if $u \\in N(v)$. $c$ is centrality vector and $\\lambda$ is eigenvalue.\n",
    "\n",
    "By Perron-Frobenius Theorem, The largest eigenvalue $\\lambda_{max}$ is always positive and unique. And The eigenvector $c_{max}$  corresponding to $\\lambda_{max}$ is used for centrality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db007a2d-8d89-4e59-b0a3-f12a90d91802",
   "metadata": {},
   "source": [
    "#### Betweenness centrality\n",
    "\n",
    "The intution behind betweenness centrality is that a node is important if it lies on many shortest paths between other nodes. Mathematically, It is defined as follows:\n",
    "\n",
    "$$c_v = \\sum_{s\\not =v\\not ={t}}\\frac{n_1}{n_2}$$\n",
    "\n",
    "Where $n_1$ is the number of shorted paths between $s$ and $t$ that contains $v$. and $n_2$ is the number of shorted paths between $s$ and $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6d8ff-5750-488a-a8bd-6f9278cd6a3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Closeness centrality\n",
    "\n",
    "The intution behind closeness centrality is that a node is important if it has small shortest path lengths to all other nodes. Mathematically, It is defined as follows:\n",
    "\n",
    "$$c_v = \\frac{1}{\\sum_{u\\not =v}d_{uv}}$$\n",
    "\n",
    "Where $d_{uv}$ is the shorest path length betwen $u$ and $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0489707-a9c0-4028-affe-dd7b77fabbce",
   "metadata": {},
   "source": [
    "### Clustering Coefficient\n",
    "\n",
    "Clustering Coefficient measures how connected $v$'s neighboring nodes are. Mathematically, It is defined as follows:\n",
    "\n",
    "$$e_v = \\frac{n}{k_v \\choose 2}$$\n",
    "\n",
    "Where $n$ is the number of edges among neighboring nodes.$k_v \\choose 2$ means choose 2 nodes among $k_v$ neighboring nodes for node for node $v$, i.e. it is the number of node pairs among the neighboring nodes for node for node $v$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bad78-6295-4d44-b3ae-50548107f449",
   "metadata": {},
   "source": [
    "### Graphlet Degree Vector\n",
    "\n",
    "Clustering Coefficient essentially counts the number of triangles in the ego-network. Here ego-network of a given node refers a subgraph induced by the node itself and its neighbors. It is basically its degree-1 neighborhood network. \n",
    "\n",
    "We can generalize clustering coefficient by counting the number of pre-specified subgraphs, i.e., graphlets.\n",
    "\n",
    "For a given node $v$, graphlets are small subgraphs that describe the structure of node $v$'s network neighborhood. \n",
    "\n",
    "To formally define graphlets, let's introduce the following 2 concepts:\n",
    "\n",
    "- ***Induced Subgraph*** is another graph, formed from a subset of vertices and all the edges connecting the vertices in the subsets.\n",
    "\n",
    "- ***Graph Isomorphism*** two graphs which contains the same number of nodes connected in the same way are said to be isomorphic. \n",
    "\n",
    "**Graphlets** are rooted connected induced non-isomorphic subgraphs. \n",
    "\n",
    "**Graphlet Degree Vector (GDV)** is a graphlet-based feature which count the number of graphlets for a given node. It provides a measure of **a node's local network topology**. It allows more detailed and comprehensive measure of local topological similarity than node degrees or clustering coefficent. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb674fa-80eb-4ae5-a290-17ce056973c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Link Prediction Tasks and Features\n",
    "\n",
    "Link Prediction is to predict new links based on the existing links. At test time, node pairs (with no existing links) are ranked, and top $K$ node pairs are predicted. The key is to design features for a pair of nodes.\n",
    "\n",
    "Two formulations of the link prediction task:\n",
    "- **Links missing at random**: remove a random set of links and then aim to predict them. \n",
    "- **Links over time**: given a graph $G[t_{0},t_{0}^{'}]$ defined by edges up to time $t_{0}^{'}$ output a ranked List L of edges that not in $G[t_{0},t_{0}^{'}]$ that are predicted to appear in time $G[t_{1},t_{1}^{'}]$. To evaluate, count how many edges in $L$ are real new edge that appear during the test period.\n",
    "\n",
    "Link prediction via proximity takes the following steps:\n",
    "1. For each pair of nodes $(x,y)$ compute score $c(x,y)$ For example, $c(x,y)$ could be the # of common neighbors of $x$ and $y$\n",
    "2. Sort pairs $(x,y)$ by the decreasing score\n",
    "$c(x,y)$\n",
    "3. Predict top $n$ pairs as new links \n",
    "4. See which of these links actually appear in $G[t_{1},t_{1}^{'}]$\n",
    "\n",
    "The question now become what is the formulation of $c$. In other words, how do we compute a score $c(x,y)$ for a given pair of nodes $(x,y)$? $c(x,y)$ is enssentially the link-level feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba04e0f-ade6-442d-9072-cf5c26c8e316",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Distance-based Features\n",
    "One way is to use shortest-path distance between 2 nodes. However, this does not capture the degree of neighborhood overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc8880-885f-4956-a352-1adfd0954df6",
   "metadata": {},
   "source": [
    "### Local Neighborhood Overlap \n",
    "The idea here is to capture the number of neighboring nodes shared between two nodes $v_1$ and $v_2$. \n",
    "  - **Common Neighbors**\n",
    "  \n",
    "  $$N(v_1)\\cap N(v_2)$$\n",
    "  \n",
    "  - **Jaccard's Coefficient**\n",
    "  \n",
    "  $$\\frac{N(v_1)\\cap N(v_2)}{N(v_1)\\cup N(v_2)}$$\n",
    "  \n",
    "  - **Adamic-Adar Index**\n",
    "  \n",
    "   $$\\sum_{u\\in N(v_1)\\cap N(v_2))}\\frac{1}{log(k_u)}$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1f5db-b1ab-43b1-82ed-07025007d46e",
   "metadata": {},
   "source": [
    "### Global Neighborhood overlap\n",
    "\n",
    "The local neighborhood feature is always zero if the two nodes do not have any neighbors in common. However, the two nodes may still potentially be connected in the future.\n",
    "\n",
    "Global neighborhood overlap metrics resolve the limitation by considering the entire graph. \n",
    "\n",
    "**Katz index** counts the number of walks of all lengths between a given pair of nodes $v_1$ and $v_2$. Mathematically it can be defined as follows:\n",
    "\n",
    "  $$\\sum_{l=1}^{\\infty}\\beta^l \\mathbf{\\textit{P}}_{v_1v_2}^{(l)}$$\n",
    "  \n",
    "Where $\\beta^l \\in (0,1)$ is a discounting factor. $\\mathbf{\\textit{P}}_{v_1v_2}^{(l)}$ is the number of walks of lengh $l$ between $v_1$ and $v_2$.  \n",
    "\n",
    "To compute $\\mathbf{\\textit{P}}_{v_1v_2}^{(l)}$, we need to make use of the adjacency matrix of the graph $\\mathbf{\\textit{A}}$. \n",
    "\n",
    "- When $l=1$, $\\mathbf{\\textit{P}}_{v_1v_2}^{(1)}$ specifies the number of walks of length 1 (direct neighborhood) between $v_1$ and $v_2$. $\\mathbf{\\textit{P}}_{v_1v_2}^{(1)} = \\mathbf{\\textit{A}}_{v_1v_2}$ \n",
    "\n",
    "- When $l=2$, $\\mathbf{\\textit{P}}_{v_1v_2}^{(2)}$specifies the number of walks of length 2 (neighbor of neighbor) between $v_1$ and $v_2$. It can be derived by summing up the number of works across $v_1$'s neighbors as follows:\n",
    "$$\\mathbf{\\textit{P}}_{v_1v_2}^{(2)} = \\sum_i A_{v_1i}*\\mathbf{\\textit{P}}_{v_1v_2}^{(1)} = \\mathbf{\\textit{A}}_{v_1v_2}^2$$\n",
    "\n",
    "As such, $\\mathbf{\\textit{A}}_{v_1v_2}^l $ specifies the number of walks of length $l$ **Katz index** can be write as \n",
    "\n",
    "  $$\\sum_{l=1}^{\\infty}\\beta^l \\mathbf{\\textit{A}}_{v_1v_2}^l$$\n",
    "  \n",
    "With this, it can be computed in closed-form\n",
    "\n",
    "\n",
    " $$S = \\sum_{l=1}^{\\infty}\\beta^l \\mathbf{\\textit{A}}^l = (I-\\beta\\mathbf{\\textit{A}})^{-1} -I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7b57f-9a04-485f-b4b6-1a6b0e749805",
   "metadata": {},
   "source": [
    "## Graph-Level Features and Graph Kernels\n",
    "\n",
    "The graph-level features are designed to characterize the structure of the entire graph.  Kernel methods are widely used for traditional ML for graph level prediction.\n",
    "\n",
    "In the case of Natural Language Processing, **Bag-of-Words(BoW)** uses word counts as features for documents without consider the ordering of the words.  BOW for a graph then would be to treat nodes as words. However, because it only focus on the node itself by ignoring the links of hte graph, it may lead to same representation for different graph.\n",
    "\n",
    "To address that, we can use **bag of node degrees**. Node degree combines the information of links and nodes and thus the structure of the graph. As such, it will produce different features for different graphs. \n",
    "\n",
    "Both **Graphlet Kernel** and **Weisfeiler-Lehman (WL) Kernel** use Bag-of-* representation of graph, where * is more sophisticated than node degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c3dc2-4dce-48c7-90f1-16138905a0df",
   "metadata": {},
   "source": [
    "### Graphlet Kernel\n",
    "\n",
    "\n",
    "Graph-level graphlet features count the number of different graphlets in a graph. \n",
    "\n",
    "It is worth noting that nodes in graphlets here do not need to be connected. In other words, isolated nodes are allowed. And unlike the node-level graphlet, graph-level graphlet are not rooted.\n",
    "\n",
    "Given graph $G$, and a graphlet list $\\mathcal{G}_k=(g_1,g_2,...,g_{n_k})$, define the graphlet count\n",
    "vector $\\mathcal{f}_G \\in \\mathbb{R}^{n_k}$ as\n",
    "\n",
    "$$(\\mathcal{f}_G)_i = \\#(g_i \\subseteq G)$$ for $$ i = 1,2,..,n_k$$\n",
    "\n",
    "Given two graphs, $G$ and $G'$, graphlet kernel is computed as\n",
    "\n",
    "$$K(G,G')={\\mathcal{f}_G}^T\\mathcal{f}_{G'} $$\n",
    "\n",
    "When $G$ and $G'$ have different sizes, it can will greatly skew the value. To address this, we can normalize each feature vector:\n",
    "\n",
    "$$K(G,G')={\\mathcal{h}_G}^T\\mathcal{h}_{G'} $$\n",
    "\n",
    "Where $\\mathcal{h}_G = \\frac{\\mathcal{f}_G}{\\sum{\\mathcal{f}_G}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da341071-d5bc-49fc-80e6-6f60f7b087cb",
   "metadata": {},
   "source": [
    "### Weisfeiler-Lehman (WL) Kernel\n",
    "\n",
    "When using graphlet kernel, counting graphlets is expensive. \n",
    "- Counting size-$k$ graphlets for a graph with size $n$ by enumeration takes $n^k$. \n",
    "- This is unavoidable in the worst-case since subgraph isomorphism test (judging whether a graph is a subgraph of another graph) is NP-hard. \n",
    "- If a graph’s node degree is bounded by $n$ , an $O(nd^{k-1})$  algorithm exists to count all the graphlets of size $k$.\n",
    "\n",
    "Can we design a more efficient graph kernel? \n",
    "\n",
    "***Weisfeiler-Lehman (WL) Kernel*** use neighborhood structure to iteratively enrich node vocabulary. It uses a generalized version of bag of node degrees. It can be achieved with the **color refinement algorithm**. Given a graph $G$ with a set of node $V$.\n",
    "- Assgin an initial color $c^{(0)}(v)$ to each node $v$.\n",
    "- Iteractively refine node colors by \n",
    "\n",
    "    $$c^{(k+1)}(v) = \\mathrm{HASH}(\\{c^{(k)}(v)\\},\\{c^{(k)}(u)\\}_{u\\in N(v)}\\})$$\n",
    "\n",
    "    where $\\mathrm{HASH}$ maps different inputs to different colors.\n",
    "\n",
    "- After $K$ steps of color refinement, $c^{(k)}(v)$ summarizes the structure of $K-hop$ neighborhood.\n",
    "\n",
    "After color refinement, WL kernel counts number of nodes with a given color. The WL kernel value is computed by the inner product of the color count vectors.\n",
    "\n",
    "WL kernel is computationally efficient.\n",
    "- The time complexity for color refinement is $O(K*n_{edge})$ where $n_{edge}$ is the number of edges in the graph.\n",
    "- The time complexity for kernel value computing is $O(n_{node})$ where $n_{node}$ is the number of nodes in the graph.\n",
    "- In total, time complexity is linear in the numebr of edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c0f37-df2c-44ae-b44f-dad6031f6681",
   "metadata": {
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "\\[1\\][Featuer Engieering Youtube. Stanford CS224W: Machine Learning with Graphs | 2021](https://www.youtube.com/watch?v=P-m1Qv6-8cI&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=1)\n",
    "\n",
    "\\[2\\][Featuer Engieering Slides. Stanford CS224W: Machine Learning with Graphs | 2021](http://web.stanford.edu/class/cs224w/slides/02-tradition-ml.pdf)\n",
    "\n",
    "\\[3\\][传统图机器学习的特征工程-节点【斯坦福CS224W】](https://www.bilibili.com/video/BV1HK411175s/?spm_id_from=333.788&vd_source=fccde7883fbf93ac15b03dee298f9f18)\n",
    "\n",
    "\\[4\\][传统图机器学习的特征工程-连接【斯坦福CS224W】](https://www.bilibili.com/video/BV1r3411m7sD/?spm_id_from=333.788)\n",
    "\n",
    "\\[5\\][传统图机器学习的特征工程-全图【斯坦福CS224W】](https://www.bilibili.com/video/BV1r3411m7sD/?spm_id_from=333.788)\n",
    "\n",
    "[6] Shervashidze, Nino, et al. \"Efficient graphlet kernels for large graph comparison.\" Artificial Intelligence and Statistics. 2009.\n",
    "\n",
    "[7] Shervashidze, Nino, et al. \"Weisfeiler-lehman graph kernels.\" Journal of Machine Learning Research 12.9 (2011).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b559b-b112-41c4-9ccb-b89458cba642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
