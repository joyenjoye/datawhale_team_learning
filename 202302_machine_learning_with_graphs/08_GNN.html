
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Task 8: Graph Neural Networks - GNN &#8212; Datawhale Team Learning Notes</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Task 7: Label Propagation for Node Classification" href="07_label_propagation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Datawhale Team Learning Notes</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome ðŸ¤—
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Image Classification
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../202301_image_classification/01_data_preparation.html">
   Task 1: Image Dataset Preparation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../202301_image_classification/02_inference_with_pretrained_models.html">
   Task 2: Inference with Pre-trained models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning with Graphs
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_introduction.html">
   Task 1: Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_traditional_feature_based_methods.html">
   Task 2: Traditional Feature-based Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_networkx.html">
   Task 3: Network Analysis with NetworkX
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_node_embedding.html">
   Task 4: Node Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_papers.html">
   Task 5: Node2vec &amp; DeepWalk
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_page_rank.html">
   Task 6: Link Analysis with PageRank
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_label_propagation.html">
   Task 7: Label Propagation for Node Classification
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Task 8: Graph Neural Networks - GNN
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/joyenjoye/datawhale_team_learning/master?urlpath=tree/docs/202302_machine_learning_with_graphs/08_GNN.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/joyenjoye/datawhale_team_learning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/joyenjoye/datawhale_team_learning/issues/new?title=Issue%20on%20page%20%2F202302_machine_learning_with_graphs/08_GNN.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/202302_machine_learning_with_graphs/08_GNN.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics-of-deep-learning">
   Basics of Deep Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning-as-optimization">
     Machine Learning as Optimization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#objective-function">
       Objective Function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-optimize">
       How to Optimize?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-function">
     Linear Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layer-linear-network">
     2-Layer Linear Network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multilayer-perceptron">
     Multilayer Perceptron
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning-for-graphs">
   Deep Learning for Graphs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-networks">
     Convolutional Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#permutation-invariance">
     Permutation Invariance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#permutation-equivariance">
     Permutation Equivariance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-neural-network">
     Graph Neural Network
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-convolutional-networks">
   Graph Convolutional Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-train">
     How to train
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gnn-vs-cnn-vs-transformer">
   GNN vs. CNN vs. Transformer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-single-layer-of-a-gnn">
   A Single Layer of a GNN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gnn-layers-in-practice">
   GNN Layers in Practice
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-normalization">
     Batch Normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     Dropout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation">
     Activation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacking-layers-of-a-gnn">
   Stacking Layers of a GNN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-manipulation-in-gnn">
   Graph Manipulation in GNN
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Task 8: Graph Neural Networks - GNN</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics-of-deep-learning">
   Basics of Deep Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#machine-learning-as-optimization">
     Machine Learning as Optimization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#objective-function">
       Objective Function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-optimize">
       How to Optimize?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-function">
     Linear Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layer-linear-network">
     2-Layer Linear Network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multilayer-perceptron">
     Multilayer Perceptron
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning-for-graphs">
   Deep Learning for Graphs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolutional-networks">
     Convolutional Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#permutation-invariance">
     Permutation Invariance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#permutation-equivariance">
     Permutation Equivariance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-neural-network">
     Graph Neural Network
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-convolutional-networks">
   Graph Convolutional Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-to-train">
     How to train
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gnn-vs-cnn-vs-transformer">
   GNN vs. CNN vs. Transformer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-single-layer-of-a-gnn">
   A Single Layer of a GNN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gnn-layers-in-practice">
   GNN Layers in Practice
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-normalization">
     Batch Normalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropout">
     Dropout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation">
     Activation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacking-layers-of-a-gnn">
   Stacking Layers of a GNN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graph-manipulation-in-gnn">
   Graph Manipulation in GNN
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="task-8-graph-neural-networks-gnn">
<h1>Task 8: Graph Neural Networks - GNN<a class="headerlink" href="#task-8-graph-neural-networks-gnn" title="Permalink to this headline">#</a></h1>
<p>In Task 2, we learnt traditional feature based methods - for a given input graph, node, link and graph-level features are extracted so that they can be feed into a model (SVM, neural network) that maps features to target labels.</p>
<p>In Task 4, we learnt graph representation learning which learns task-idependent features for downstream models efficiently. It uses a <em><strong>shallow</strong></em> <strong>Encoder</strong> to map nodes to emebdings and <strong>Decoder</strong> to map embeddings to similarity Score.</p>
<p>The limitation of shallow emebedding methods are as follows:</p>
<ul class="simple">
<li><p>The complexity of <span class="math notranslate nohighlight">\(O(|V|)\)</span> as there is no sharing of paramters between nodes, and every nodes has its own unique embedding</p></li>
<li><p>Inherently transductive and cannot generte emebeddings that not seen during training.</p></li>
<li><p>Node features are not incorporated.</p></li>
</ul>
<p>In this task, we learn how to use deep learning methods (graph neural networks, GNNs) to get a deep encoder to map nodes to embeddings.</p>
<p>Essentially, the deep encoder is a multiple layers of non-linear transformations based on graph structure.</p>
<section id="basics-of-deep-learning">
<h2>Basics of Deep Learning<a class="headerlink" href="#basics-of-deep-learning" title="Permalink to this headline">#</a></h2>
<section id="machine-learning-as-optimization">
<h3>Machine Learning as Optimization<a class="headerlink" href="#machine-learning-as-optimization" title="Permalink to this headline">#</a></h3>
<section id="objective-function">
<h4>Objective Function<a class="headerlink" href="#objective-function" title="Permalink to this headline">#</a></h4>
<p>Formulate the task as an optimization probelm. Here, we optimize <span class="math notranslate nohighlight">\(\Theta\)</span> to minize the objective function <span class="math notranslate nohighlight">\(\mathcal{L}(\mathbf{y},f(\mathbf{x}))\)</span>.</p>
<div class="math notranslate nohighlight">
\[\min_{\Theta} \mathcal{L}(\mathbf{y},f(\mathbf{x}))\]</div>
<p>Where <span class="math notranslate nohighlight">\(\Theta\)</span> contains parameters of <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>The <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">Loss function</a> <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> can take many forms: L1, L2, huber, max margin(hinge loss), cross entropy, and etc. One of the common loss for classification is cross entropy loss.</p>
<p>Consider we are doing multi-class classification where the target variable can belong to one of 3 classes: Class 1, Class 2, Class 3.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Target Variable</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>Class 1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>Class 3</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>Class 2</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>Class 2</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>Class 1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p></p></td>
</tr>
</tbody>
</table>
<p>One-hot encoding is applied on the target variable, we get <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> which is a n by 3 matrix. where <span class="math notranslate nohighlight">\({y}_i\)</span> is the actual values of the i-th class for a given instance.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Class 1 (<span class="math notranslate nohighlight">\(y_1\)</span>)</p></th>
<th class="text-align:center head"><p>Class 2 (<span class="math notranslate nohighlight">\(y_2\)</span>)</p></th>
<th class="text-align:center head"><p>Class (<span class="math notranslate nohighlight">\(y_3\)</span>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p></p></td>
</tr>
</tbody>
</table>
<p>We want to train a model <span class="math notranslate nohighlight">\(\hat {\mathbf{y}} = f(\mathbf{x})\)</span> to make prediction on the probablity of each class. <span class="math notranslate nohighlight">\(\hat {{y}}_i\)</span> is the predicted values of the i-th class for a given instance.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Class 1 (<span class="math notranslate nohighlight">\(\hat{y}_1\)</span>)</p></th>
<th class="text-align:center head"><p>Class 2 (<span class="math notranslate nohighlight">\(\hat {y}_2\)</span>)</p></th>
<th class="text-align:center head"><p>Class (<span class="math notranslate nohighlight">\(\hat {y}_3\)</span>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p><span style="color:blue">0.80</span></p></td>
<td class="text-align:center"><p>0.11</p></td>
<td class="text-align:center"><p>0.09</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>0.05</p></td>
<td class="text-align:center"><p>0.05</p></td>
<td class="text-align:center"><p><span style="color:blue">0.99</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>0.20</p></td>
<td class="text-align:center"><p><span style="color:blue">0.70</span></p></td>
<td class="text-align:center"><p>0.10</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>0.10</p></td>
<td class="text-align:center"><p><span style="color:blue">0.88</span></p></td>
<td class="text-align:center"><p>0.02</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p><span style="color:blue">0.65</span></p></td>
<td class="text-align:center"><p>0.25</p></td>
<td class="text-align:center"><p>0.10</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p></p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p></p></td>
</tr>
</tbody>
</table>
<p>The sum of the probability of each class equals to 1. For a given instance, we can then predict it belongs to the class with maximum probablity. In general, to ensure the output of <span class="math notranslate nohighlight">\(f(x)\)</span> to be probabilities which sum up to 1, softmax function <span class="math notranslate nohighlight">\(\sigma\)</span> is applied on the output <span class="math notranslate nohighlight">\(g(x)\)</span> from the previous step:</p>
<div class="math notranslate nohighlight">
\[\hat{{y}} = f(x) = \sigma\big(g(x)\big)\]</div>
<p>The predicted values of the <span class="math notranslate nohighlight">\(i\)</span>-th class for a given instance <span class="math notranslate nohighlight">\(\hat{{y}}_i\)</span> is thus as follows</p>
<div class="math notranslate nohighlight">
\[ \hat{y}_i =f(x)_i = \sigma\big(g(x)_i\big) = \frac{e^{g(x)_i}}{ \sum_{i=j}^C e^{g(x)_j}}\]</div>
<p>Where <span class="math notranslate nohighlight">\(C\)</span> is the number of classes. In the above example <span class="math notranslate nohighlight">\(C=3\)</span></p>
<p>The cross entropy loss for each instance is thus:</p>
<div class="math notranslate nohighlight">
\[\mathrm{CE}\big(y,f(x)\big) =-\sum_{i=1}^C {y}_i \hat {{y}}_i =-\sum_{i=1}^C {y}_i logf({x})_i\]</div>
<p>The lower the loss, the closer the prediction <span class="math notranslate nohighlight">\(\hat {{y}}\)</span> is to one-hot encoded true label <span class="math notranslate nohighlight">\({y}\)</span>. Sum up the loss over all training examples, we have the loss function as follows:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{y},f(\mathbf{x})) = \sum_{(x,y)\in\mathcal{T}} \mathrm{CE}\big(y,f(x)\big) \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> training set containing all pairs of data and labels</p>
</section>
<section id="how-to-optimize">
<h4>How to Optimize?<a class="headerlink" href="#how-to-optimize" title="Permalink to this headline">#</a></h4>
<p>Once we have the objective function, the next question is how to optimize it?</p>
<p><strong>Gradient Descent</strong> is an iterative algorithm which repeated update weights <span class="math notranslate nohighlight">\(\Theta\)</span> in the oppsite direction of gradients until the objective function converge.</p>
<div class="math notranslate nohighlight">
\[ \Theta \leftarrow  \Theta - \mathbf{\eta} \, \nabla_\Theta \mathcal{L} \]</div>
<p>Where <span class="math notranslate nohighlight">\(\eta\)</span> is a hyperparameter that controls the size of gradient step. It can vary over the course of training. The graident vector <span class="math notranslate nohighlight">\(\nabla_\Theta \mathcal{L}\)</span> can be computed as follows:</p>
<div class="math notranslate nohighlight">
\[ \nabla_\Theta \mathcal{L}= (\frac{\partial \mathcal{L}}{\partial \, {\Theta_1} } ,\frac{\partial \mathcal{L}}{\partial\,{\Theta_2} },...)\]</div>
<p>Ideally, we would like to terminate the iteration when gradient equals 0. In practice we stop training when it no longer imporves performance on validation set.</p>
<p>The problem with grident decent is that extract gradient requires computing <span class="math notranslate nohighlight">\(\nabla_\Theta \mathcal{L}(\mathbf{y},\mathbf{x})\)</span> where <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is the entire dataset. i.e. summing up gradient contribution over all data points in the dataset. Modern dataset often contain billions of data points which leads to extermelly expensive caculation for every gradient descent step.</p>
<p>One solution to address this is to use <strong>Stochastic Gradient Descent(SGD)</strong>. At each step, it picks a different minibatch <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> containing a subset of the dataset use it a input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.  The SGD process involves the following conceps:</p>
<ul class="simple">
<li><p><em>Batch size</em>: the number of data points in a minibatch</p></li>
<li><p><em>Iteration</em>: 1 step of SGD on a minibatch</p></li>
<li><p><em>Epoch</em>: one full pass over the dataset (# iterations is equal to ratio of dataset size and batch size)</p></li>
</ul>
<p>SGD is unbiased estimator of full gradient. But there is no guarantee on the rate of convergence. In practice often requires tuning of learning rate. Common optimizer that improves over SGD: Adam, Adagrad, Adadelta, RMSprop.</p>
<p>When updating the weights <span class="math notranslate nohighlight">\(\Theta\)</span> interatively, there are 2 steps for each iteration.</p>
<ul class="simple">
<li><p><strong>Forward Propoagation</strong>: compute <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and updated <span class="math notranslate nohighlight">\(\Theta\)</span>. Use computed <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to compute <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p></li>
<li><p><strong>Back Propoagation</strong>: using chain rule to propagate gradients of intermediate steps, and finally obtain gradient <span class="math notranslate nohighlight">\(\nabla_\Theta \mathcal{L}\)</span> .</p></li>
</ul>
</section>
</section>
<section id="linear-function">
<h3>Linear Function<a class="headerlink" href="#linear-function" title="Permalink to this headline">#</a></h3>
<p>In previous section, we formulate machine learning as an optimization probelm.</p>
<div class="math notranslate nohighlight">
\[\min_{\Theta} \mathcal{L}(\mathbf{y},f(\mathbf{x}))\]</div>
<p>Now letâ€™s see try to apply and use it. To start, consider linear function.</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{x}) = W \cdot \mathbf{x}, \quad \Theta = \{W\}\]</div>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(f\)</span> returns a scalar, then <span class="math notranslate nohighlight">\(W\)</span> is a weight vector.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(f\)</span> returns a vector, then <span class="math notranslate nohighlight">\(W\)</span> is a weight matrix, called Jacobian Matrix.</p></li>
</ul>
</section>
<section id="layer-linear-network">
<h3>2-Layer Linear Network<a class="headerlink" href="#layer-linear-network" title="Permalink to this headline">#</a></h3>
<p>To make it a bit more complex, letâ€™s look at 2-Layer linear network,</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{x}) = g(h(\mathbf{x})) =W_2( W_1 \mathbf{x}), \quad \Theta = \{W_1,W_2\}\]</div>
<p>Here we use <span class="math notranslate nohighlight">\(h(\mathbf{x})= W_1 \mathbf{x}\)</span> to denote the hidden layer.</p>
<p>Assume we use L2 Loss for objective function and SGD for optimization. Then for each minibatch <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, the loss can be calculated as follows:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L} = \sum_{(x,y)\in\mathcal{B}} ||\mathbf{y}-f(\mathbf{x})||_2\]</div>
</section>
<section id="multilayer-perceptron">
<h3>Multilayer Perceptron<a class="headerlink" href="#multilayer-perceptron" title="Permalink to this headline">#</a></h3>
<p>Note that in 2-Layer Linear Network, <span class="math notranslate nohighlight">\(W_2 W_1\)</span> is just another matrix, and <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> is still linear w.r.t. <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.  To intorudce non-linearity, we need to apply a non-linear transformations. Popular non-linear transformation function include:</p>
<ul class="simple">
<li><p>Rectified Linear Unit(ReLU)
$<span class="math notranslate nohighlight">\(ReLU(x)= \max(x,0)\)</span>$</p></li>
<li><p>Sigmoid
$<span class="math notranslate nohighlight">\(\sigma(x)= \frac{1}{1+e^{-x}}\)</span>$</p></li>
</ul>
<p>Each layer of Multilayer Perceptron(MLP) combines linear and non-linear transformation.</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{(l+1)} = \sigma(W_l\mathbf{x}^{(l)}+b^{l})\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(W_l\)</span> is weight matrix that transforms hidden representation at layer <span class="math notranslate nohighlight">\(l\)</span> to layer <span class="math notranslate nohighlight">\(l+1\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(b^{l}\)</span> is bias at layer  <span class="math notranslate nohighlight">\(l\)</span> , and is added to the linear transformation of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> is non-linearity function (e.g., sigmod)</p></li>
</ul>
</section>
</section>
<section id="deep-learning-for-graphs">
<h2>Deep Learning for Graphs<a class="headerlink" href="#deep-learning-for-graphs" title="Permalink to this headline">#</a></h2>
<p>Nodes aggregate information from their neighbors using neural networks. The important consideration is how to aggregate information acorss the layers. A basic appraoch is to average neighbor messages and paply a neural network.</p>
<div class="math notranslate nohighlight">
\[h_v^{l+1} = \sigma\big(W_l \sum_{u\in N(v)}\frac{h_u^{(l)}}{|\mathrm{N}(v)|}+\mathrm{B}_l h_v^{(l)} \big), \quad \forall \, l \in \{0,...,L-1\}\]</div>
<section id="convolutional-networks">
<h3>Convolutional Networks<a class="headerlink" href="#convolutional-networks" title="Permalink to this headline">#</a></h3>
</section>
<section id="permutation-invariance">
<h3>Permutation Invariance<a class="headerlink" href="#permutation-invariance" title="Permalink to this headline">#</a></h3>
</section>
<section id="permutation-equivariance">
<h3>Permutation Equivariance<a class="headerlink" href="#permutation-equivariance" title="Permalink to this headline">#</a></h3>
</section>
<section id="graph-neural-network">
<h3>Graph Neural Network<a class="headerlink" href="#graph-neural-network" title="Permalink to this headline">#</a></h3>
<p>Graph neural networks consist of multiple permutation equivariant / invariant functions.</p>
</section>
</section>
<section id="graph-convolutional-networks">
<h2>Graph Convolutional Networks<a class="headerlink" href="#graph-convolutional-networks" title="Permalink to this headline">#</a></h2>
<p>pg 51 to 76</p>
<section id="how-to-train">
<h3>How to train<a class="headerlink" href="#how-to-train" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="gnn-vs-cnn-vs-transformer">
<h2>GNN vs. CNN vs. Transformer<a class="headerlink" href="#gnn-vs-cnn-vs-transformer" title="Permalink to this headline">#</a></h2>
</section>
<section id="a-single-layer-of-a-gnn">
<h2>A Single Layer of a GNN<a class="headerlink" href="#a-single-layer-of-a-gnn" title="Permalink to this headline">#</a></h2>
</section>
<section id="gnn-layers-in-practice">
<h2>GNN Layers in Practice<a class="headerlink" href="#gnn-layers-in-practice" title="Permalink to this headline">#</a></h2>
<section id="batch-normalization">
<h3>Batch Normalization<a class="headerlink" href="#batch-normalization" title="Permalink to this headline">#</a></h3>
</section>
<section id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">#</a></h3>
</section>
<section id="activation">
<h3>Activation<a class="headerlink" href="#activation" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="stacking-layers-of-a-gnn">
<h2>Stacking Layers of a GNN<a class="headerlink" href="#stacking-layers-of-a-gnn" title="Permalink to this headline">#</a></h2>
</section>
<section id="graph-manipulation-in-gnn">
<h2>Graph Manipulation in GNN<a class="headerlink" href="#graph-manipulation-in-gnn" title="Permalink to this headline">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./202302_machine_learning_with_graphs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="07_label_propagation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Task 7: Label Propagation for Node Classification</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Joye<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>